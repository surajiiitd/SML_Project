{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy.linalg as linalg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn\n",
    "from random import shuffle\n",
    "from numpy.random import choice\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from scipy import ndarray\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# image processing library\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "from skimage import io\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.feature import hog\n",
    "from skimage.feature import local_binary_pattern\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import matplotlib.cm as cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow.keras as keras\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(predict,true):\n",
    "    h=[]\n",
    "    for i in range(len(predict)):\n",
    "        if predict[i]==true[i]:\n",
    "            h.append(1)\n",
    "        else:\n",
    "            h.append(0)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation \n",
    "def cross_validation_gaussian(train_new,train_label2,train_size):\n",
    "\n",
    "    fold=5\n",
    "    model=[]\n",
    "    score_set=[]\n",
    "    if len(train_new)%5==0:\n",
    "        length=int(len(train_new)/5)\n",
    "    else:\n",
    "        length=int(len(train_new)/5)+1\n",
    "    newlength=length\n",
    "    counter=0\n",
    "    for q in range(fold):\n",
    "        valid_test_data=[]\n",
    "        valid_test_label=[]\n",
    "        valid_train_data=[]\n",
    "        valid_train_label=[]\n",
    "        if (max(len(train_new),length)==length):\n",
    "            length=len(train_new)\n",
    "#         print(\"Counter : \",counter)\n",
    "#         print(\"End : \",length)\n",
    "        for j in range(counter,length):\n",
    "            valid_test_data.append(train_new[j])\n",
    "            valid_test_label.append(train_label2[j])\n",
    "        counter=counter+newlength\n",
    "        length=length+newlength\n",
    "        valid_test_data=valid_test_data\n",
    "        valid_test_label=valid_test_label\n",
    "        for j in range(len(train_new)):\n",
    "            if train_new[j] not in valid_test_data:\n",
    "                valid_train_data.append(train_new[j])\n",
    "                valid_train_label.append(train_label2[j])\n",
    "#         print(q)\n",
    "#         print(\"Data : \",valid_train_data)\n",
    "#         clf = LogisticRegressionCV(random_state=0, solver='lbfgs',multi_class='multinomial').fit(copy.deepcopy(np.array(valid_train_data)), copy.deepcopy(np.array(valid_train_label)))\n",
    "        clf = GaussianNB().fit(np.array(valid_train_data), np.array(valid_train_label))\n",
    "        a=[]\n",
    "        a=clf.predict(np.array(valid_test_data))\n",
    "        score=clf.score(np.array(valid_test_data),np.array(valid_test_label))\n",
    "        score_set.append(score)\n",
    "        model.append(clf) \n",
    "        del clf\n",
    "\n",
    "\n",
    "    return model,score_set\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation \n",
    "def cross_validation_svm(train_new,train_label2,train_size):\n",
    "\n",
    "    fold=5\n",
    "    model=[]\n",
    "    score_set=[]\n",
    "    if len(train_new)%5==0:\n",
    "        length=int(len(train_new)/5)\n",
    "    else:\n",
    "        length=int(len(train_new)/5)+1\n",
    "    newlength=length\n",
    "    counter=0\n",
    "    for q in range(fold):\n",
    "        valid_test_data=[]\n",
    "        valid_test_label=[]\n",
    "        valid_train_data=[]\n",
    "        valid_train_label=[]\n",
    "        if (max(len(train_new),length)==length):\n",
    "            length=len(train_new)\n",
    "#         print(\"Counter : \",counter)\n",
    "#         print(\"End : \",length)\n",
    "        for j in range(counter,length):\n",
    "            valid_test_data.append(train_new[j])\n",
    "            valid_test_label.append(train_label2[j])\n",
    "        counter=counter+newlength\n",
    "        length=length+newlength\n",
    "        valid_test_data=valid_test_data\n",
    "        valid_test_label=valid_test_label\n",
    "        for j in range(len(train_new)):\n",
    "            if train_new[j] not in valid_test_data:\n",
    "                valid_train_data.append(train_new[j])\n",
    "                valid_train_label.append(train_label2[j])\n",
    "#         print(q)\n",
    "#         print(\"Data : \",valid_train_data)\n",
    "#         clf = LogisticRegressionCV(random_state=0, solver='lbfgs',multi_class='multinomial').fit(copy.deepcopy(np.array(valid_train_data)), copy.deepcopy(np.array(valid_train_label)))\n",
    "#         clf = GaussianNB().fit(np.array(valid_train_data), np.array(valid_train_label))\n",
    "        clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "        clf.fit(np.array(valid_train_data), np.array(valid_train_label))\n",
    "        a=[]\n",
    "        a=clf.predict(np.array(valid_test_data))\n",
    "        score=clf.score(np.array(valid_test_data),np.array(valid_test_label))\n",
    "        score_set.append(score)\n",
    "        model.append(clf) \n",
    "        del clf\n",
    "\n",
    "\n",
    "    return model,score_set\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation \n",
    "def cross_validation_logistic(train_new,train_label2,train_size):\n",
    "\n",
    "    fold=5\n",
    "    model=[]\n",
    "    score_set=[]\n",
    "    if len(train_new)%5==0:\n",
    "        length=int(len(train_new)/5)\n",
    "    else:\n",
    "        length=int(len(train_new)/5)+1\n",
    "    newlength=length\n",
    "    counter=0\n",
    "    for q in range(fold):\n",
    "        valid_test_data=[]\n",
    "        valid_test_label=[]\n",
    "        valid_train_data=[]\n",
    "        valid_train_label=[]\n",
    "        if (max(len(train_new),length)==length):\n",
    "            length=len(train_new)\n",
    "#         print(\"Counter : \",counter)\n",
    "#         print(\"End : \",length)\n",
    "        for j in range(counter,length):\n",
    "            valid_test_data.append(train_new[j])\n",
    "            valid_test_label.append(train_label2[j])\n",
    "        counter=counter+newlength\n",
    "        length=length+newlength\n",
    "        valid_test_data=valid_test_data\n",
    "        valid_test_label=valid_test_label\n",
    "        for j in range(len(train_new)):\n",
    "            if train_new[j] not in valid_test_data:\n",
    "                valid_train_data.append(train_new[j])\n",
    "                valid_train_label.append(train_label2[j])\n",
    "#         print(q)\n",
    "#         print(\"Data : \",valid_train_data)\n",
    "        clf = LogisticRegressionCV(random_state=0, solver='lbfgs',multi_class='multinomial').fit(np.array(valid_train_data), np.array(valid_train_label))\n",
    "#         clf = GaussianNB().fit(np.array(valid_train_data), np.array(valid_train_label))\n",
    "        a=[]\n",
    "        a=clf.predict(np.array(valid_test_data))\n",
    "        score=clf.score(np.array(valid_test_data),np.array(valid_test_label))\n",
    "        score_set.append(score)\n",
    "        model.append(clf) \n",
    "        del clf\n",
    "\n",
    "\n",
    "    return model,score_set\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict,true):\n",
    "    count=0\n",
    "    for i in range(len(predict)):\n",
    "        if predict[i]==true[i]:\n",
    "            count+=1\n",
    "    return count/float(len(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geeks for geeks \n",
    "def sort_list(list1, list2): \n",
    "  \n",
    "    zipped_pairs = zip(list2, list1) \n",
    "  \n",
    "    z = [x for _, x in sorted(zipped_pairs)] \n",
    "      \n",
    "    return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tpr_fpr(predict,real,checker):\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "#     print(\"find_tpr_fpr\")\n",
    "    fn=0\n",
    "    voc=copy.deepcopy([0,1,2,3,4,5,6])\n",
    "    v=voc.index(checker)\n",
    "    del voc[v]\n",
    "    for i in range(len(predict)):\n",
    "        if predict[i]==checker and real[i]==checker:\n",
    "            tp=tp+1\n",
    "        if (predict[i] in voc ) and real[i]==checker:\n",
    "            fn=fn+1\n",
    "        if predict[i]==checker and (real[i] in voc):\n",
    "            fp=fp+1\n",
    "        if (predict[i] in voc) and (real[i] in voc):\n",
    "            tn=tn+1\n",
    "    tpr2=0\n",
    "    fpr2=0\n",
    "#     print(\"Total :\",(tp+fp+tn+fn))\n",
    "    tpr2=float(tp/float(tp+fn))   \n",
    "    fpr2=float(fp/float(fp+tn))\n",
    "    \n",
    "    return tpr2,fpr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_design(prob_dist,testdata,checker):\n",
    "    aux1=[]\n",
    "    aux2=[]\n",
    "    testdata1=copy.deepcopy(testdata)\n",
    "    for i in range(len(testdata)):\n",
    "        \n",
    "        aux1.append(prob_dist[i])\n",
    "        aux2.append(testdata[i])\n",
    "    main1=sort_list(aux2, aux1)\n",
    "#     print(\"Probability in incresing order : \",main1)\n",
    "    \n",
    "    tpr=[]\n",
    "    fpr=[]\n",
    "    #aux1 has prob_distribution and main1 has testlabel in sorted order\n",
    "   \n",
    "    main2=[]\n",
    "    j=0\n",
    "    for j in range(len(prob_dist)):\n",
    "        main2.append(checker)\n",
    "    i=0\n",
    "    #Logic \n",
    "    if (checker+1)==7:\n",
    "        flag=checker-1\n",
    "    else:\n",
    "        flag=checker+1\n",
    "        \n",
    "    while i <len(prob_dist):\n",
    "        tpr1=0\n",
    "        fpr1=0\n",
    "        j=0\n",
    "        \n",
    "        while (j  <= i):\n",
    "            main2[j]=flag\n",
    "            j=j+1\n",
    "#         j=i\n",
    "#         while j <len(prob_dist):\n",
    "#             main1[j]=2\n",
    "#             j=j+1\n",
    "#         print(main1)\n",
    "#         m=[]\n",
    "#         tpr.append(tpr1)\n",
    "#         fpr.append(fpr1)\n",
    "        tpr1,fpr1=find_tpr_fpr(copy.deepcopy(main2),copy.deepcopy(main1),checker)\n",
    "#         e.append(testdata)\n",
    "#         d.append(main1)\n",
    "        fpr.append(fpr1)\n",
    "        tpr.append(tpr1)\n",
    "        \n",
    "        i=i+50\n",
    "    return tpr,fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/suraj18025/SML_Project/fer2013'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('SML_Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fer2013'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bea079530227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fer2013'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fer2013'"
     ]
    }
   ],
   "source": [
    "os.chdir('fer2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"fer2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35887"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['emotion'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage=df[\"Usage\"].tolist()\n",
    "data=df[\"pixels\"].tolist()\n",
    "train_label1=df[\"emotion\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "train_label=[]\n",
    "test_data=[]\n",
    "test_label=[]\n",
    "for i in range(len(data)):\n",
    "    if usage[i]==\"Training\":\n",
    "        g=data[i].split(' ')\n",
    "        h=[int(i) for i in g]\n",
    "        train_data.append(h)\n",
    "        train_label.append(train_label1[i])\n",
    "    else:\n",
    "      \n",
    "        g=data[i].split(' ')\n",
    "        h=[int(i) for i in g]\n",
    "        test_data.append(h)\n",
    "        test_label.append(train_label1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "t=[0]*7\n",
    "for i in range(len(train_data)):\n",
    "    t[train_label[i]]+=1\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_local=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Augmentation Code from https://gist.github.com/tomahim/9ef72befd43f5c106e592425453cb6ae\n",
    "# def random_rotation(image_array: ndarray):\n",
    "#     # pick a random degree of rotation between 25% on the left and 25% on the right\n",
    "#     random_degree = random.uniform(-25, 25)\n",
    "#     return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "# def random_noise(image_array: ndarray):\n",
    "#     # add random noise to the image\n",
    "#     return sk.util.random_noise(image_array)\n",
    "\n",
    "# def horizontal_flip(image_array: ndarray):\n",
    "#     # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n",
    "#     return image_array[:, ::-1]\n",
    "\n",
    "# # dictionary of the transformations we defined earlier\n",
    "# available_transformations = {\n",
    "#     'rotate': random_rotation,\n",
    "#     'noise': random_noise,\n",
    "#     'horizontal_flip': horizontal_flip\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = 'images/cat'\n",
    "# num_files_desired = 10\n",
    "\n",
    "# # find all files paths from the folder\n",
    "# images = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# num_generated_files = 0\n",
    "# while num_generated_files <= num_files_desired:\n",
    "#     # random image from the folder\n",
    "#     image_path = random.choice(images)\n",
    "#     # read image as an two dimensional array of pixels\n",
    "#     image_to_transform = sk.io.imread(image_path)\n",
    "#     # random num of transformation to apply\n",
    "#     num_transformations_to_apply = random.randint(1, len(available_transformations))\n",
    "\n",
    "#     num_transformations = 0\n",
    "#     transformed_image = None\n",
    "#     while num_transformations <= num_transformations_to_apply:\n",
    "#         # random transformation to apply for a single image\n",
    "#         key = random.choice(list(available_transformations))\n",
    "#         transformed_image = available_transformations[key](image_to_transform)\n",
    "#         num_transformations += 1\n",
    "\n",
    "# new_file_path = '%s/augmented_image_%s.jpg' % (folder_path, num_generated_files)\n",
    "\n",
    "# # write image to the disk\n",
    "# io.imsave(new_file_path, transformed_image)\n",
    "# num_generated_files += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = load_img('data/train/cats/cat.0.jpg')  # this is a PIL image\n",
    "# x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "choice=6\n",
    "#value of i > for choice=0,1,2,3,4,5,6 [0,16,0,0,0,1,0]\n",
    "#Augmenting the train_data\n",
    "for j in range(len(train_data)):\n",
    "    if train_label[j]==choice:\n",
    "        x=np.array(train_data[j]).reshape(48,48)\n",
    "        y=[]\n",
    "        y.append(x)\n",
    "        y.append(x)\n",
    "        y.append(x)\n",
    "        y=np.swapaxes(np.array(y), 0, 2)\n",
    "        x=copy.deepcopy(np.array(y))\n",
    "        x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "        # the .flow() command below generates batches of randomly transformed images\n",
    "        # and saves the results to the `preview/` directory\n",
    "\n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1,\n",
    "                                  save_to_dir='preview', save_prefix='emotion', save_format='jpeg'):\n",
    "            i += 1\n",
    "            if i > 2:\n",
    "#                 print(\"something\")\n",
    "                break  # otherwise the generator would loop indefinitely\n",
    "        os.chdir('preview')\n",
    "        \n",
    "        lists=os.listdir()\n",
    "        for k in lists:\n",
    "            img=cv2.imread(k,1)\n",
    "            img=img[:,:,0].ravel().tolist()\n",
    "            train_data.append(img)\n",
    "            train_label.append(choice)\n",
    "            \n",
    "        for k in lists:\n",
    "            os.remove(k)\n",
    "        os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1=copy.deepcopy(train_data[:28709])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=28709\n",
    "while i<len(train_data):\n",
    "    if i%5000==0:\n",
    "        print(\"Counter : \",i)\n",
    "    a=np.rot90(np.array(train_data[i]).reshape(48,48))\n",
    "    b=np.rot90(np.array(a))\n",
    "    c=np.rot90(np.array(b))\n",
    "    train_data1.append(c.ravel().tolist())\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=copy.deepcopy(train_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(train,label):\n",
    "    count=0\n",
    "    temp=[]\n",
    "    for i in range(len(list(set(label)))):\n",
    "        temp.append([])\n",
    "    for i in range(len(train_data)):\n",
    "        temp[train_label[i]].append(train_data[i])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=6000\n",
    "data=retrieve(train_data,train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[]\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    shuffled_data=random.sample(data[i], len(data[i]))\n",
    "    X_train=shuffled_data[:size]\n",
    "    temp.append(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "for i in temp:\n",
    "    for j in i:\n",
    "        train_data.append(j)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=[]\n",
    "for j in range(7):\n",
    "    for i in range(6000):\n",
    "        train_label.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Shuffling of data\n",
    "main_data_set=[]\n",
    "for i in range(len(train_data)):\n",
    "\n",
    "    temp=[]\n",
    "    temp.append(train_data[i])\n",
    "    temp.append(train_label[i])\n",
    "    main_data_set.append(temp)\n",
    "X_train=random.sample(main_data_set, len(main_data_set))\n",
    "train_data=[]\n",
    "train_label=[]\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append(X_train[i][0])\n",
    "    train_label.append(X_train[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For loading the train data and train label\n",
    "train_data = pickle.load(open(\"train_data_fer_6000perclass.txt\",\"rb\"))\n",
    "train_label= pickle.load(open(\"train_label_fer_6000perclass.txt\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Dumping into train data and label.\n",
    "# with open(\"train_data_fer_6000perclass.txt\", 'wb') as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "# with open(\"train_label_fer_6000perclass.txt\", 'wb') as f:\n",
    "#     pickle.dump(train_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of Data \n",
    "def fashion_scatter(x, colors):\n",
    "    # choose a color palette with seaborn.\n",
    "    num_classes = len(np.unique(colors))\n",
    "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # add the labels for each digit corresponding to the label\n",
    "    txts = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "\n",
    "        # Position of each label at median of data points.\n",
    "\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Visualisation of Data \n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=8)\n",
    "# train_data1=pca.fit_transform(np.array(train_data))\n",
    "# tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "# tsne_results = tsne.fit_transform(np.array(train_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion_scatter(tsne_results, np.array(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=list(train_data)\n",
    "# train_label=list(train_label)\n",
    "# test_data=list(test_data)\n",
    "# test_label=list(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=5\n",
    "train_size=len(train_data)\n",
    "valid_size=int(train_size/float(fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################\n",
    "\n",
    "# main_train_data=copy.deepcopy(train_data)\n",
    "# main_test_data=copy.deepcopy(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=copy.deepcopy(main_train_data)\n",
    "# test_data=copy.deepcopy(main_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LBP feature \n",
    "radius=3\n",
    "n_points=8*radius\n",
    "METHOD=\"uniform\"\n",
    "train_data1=[]\n",
    "for i in range(len(train_data)):\n",
    "    gray=np.array(train_data[i],dtype = np.uint8).reshape(48,48)\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, METHOD)\n",
    "    train_data1.append(lbp.ravel().tolist())\n",
    "test_data1=[]\n",
    "for i in range(len(test_data)):\n",
    "    gray=np.array(test_data[i],dtype = np.uint8).reshape(48,48)\n",
    "   \n",
    "    lbp = local_binary_pattern(gray, n_points, radius, METHOD)\n",
    "    test_data1.append(lbp.ravel().tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grayscaling features\n",
    "# for i in range(len(train_data)):\n",
    "#     main_train_data[i]+=train_data[i]\n",
    "# for i in range(len(test_data1)):\n",
    "#     main_test_data[i]+=test_data[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hog Feature\n",
    "train_data1=[]\n",
    "for i in range(len(train_data)):\n",
    "    gray=np.array(train_data[i]).reshape(48,48)\n",
    "    fd, hog_image = hog(gray, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2), visualize=True)\n",
    "    train_data1.append(fd)\n",
    "test_data1=[]\n",
    "for i in range(len(test_data)):\n",
    "    gray=np.array(test_data[i]).reshape(48,48)\n",
    "    fd, hog_image = hog(gray, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2), visualize=True)\n",
    "    test_data1.append(fd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA \n",
    "pca = PCA(n_components=30)\n",
    "train_data=pca.fit_transform(np.array(train_data1))\n",
    "test_data=pca.transform(np.array(test_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=[]\n",
    "# for i in range(len(train_data)):\n",
    "#     x=np.array(train_data[i]).reshape(48,48).tolist()\n",
    "#     y=np.array(x,dtype=np.uint8)\n",
    "#     y=cv2.resize(y,(224,224)).tolist()\n",
    "#     temp=[]\n",
    "#     temp.append(y)\n",
    "#     temp.append(y)\n",
    "#     temp.append(y)\n",
    "#     y=np.swapaxes(np.array(temp), 0, 2)\n",
    "#     train.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=[]\n",
    "# for i in range(len(test_data)):\n",
    "#     x=np.array(test_data[i]).reshape(48,48).tolist()\n",
    "#     y=np.array(x,dtype=np.uint8)\n",
    "#     y=cv2.resize(y,(224,224)).tolist()\n",
    "#     temp=[]\n",
    "#     temp.append(y)\n",
    "#     temp.append(y)\n",
    "#     temp.append(y)\n",
    "#     y=np.swapaxes(np.array(temp), 0, 2)\n",
    "#     test.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"try1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing into directory named as data\n",
    "s=\"Image@\"\n",
    "g=\".jpg\"\n",
    "counter=0\n",
    "for i in range(len(train_data)):\n",
    "    counter+=1\n",
    "    f=os.listdir()\n",
    "    if str(train_label[i]) not in f:\n",
    "        os.mkdir(str(train_label[i]))\n",
    "        img=np.array(train_data[i],dtype=np.uint8).reshape(48,48)\n",
    "        os.chdir(str(train_label[i]))\n",
    "#         plt.imshow(np.array(train_data[i]).reshape(48,48))\n",
    "        cv2.imwrite(s+str(counter)+g,img/np.max(img)*255)\n",
    "        os.chdir('..')\n",
    "    else:\n",
    "        img=np.array(train_data[i],dtype=np.uint8).reshape(48,48)\n",
    "        os.chdir(str(train_label[i]))\n",
    "        cv2.imwrite(s+str(counter)+g,img/np.max(img)*255)\n",
    "        os.chdir('..')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('../try2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing into directory named as testdata\n",
    "s=\"Image#\"\n",
    "g=\".jpg\"\n",
    "counter=0\n",
    "for i in range(len(test_data)):\n",
    "    counter+=1\n",
    "    f=os.listdir()\n",
    "    if str(test_label[i]) not in f:\n",
    "        os.mkdir(str(test_label[i]))\n",
    "        img=np.array(test_data[i],dtype=np.uint8).reshape(48,48)\n",
    "        os.chdir(str(test_label[i]))\n",
    "#         plt.imshow(np.array(train_data[i]).reshape(48,48))\n",
    "        cv2.imwrite(s+str(counter)+g,img/np.max(img)*255)\n",
    "        os.chdir('..')\n",
    "    else:\n",
    "        img=np.array(test_data[i],dtype=np.uint8).reshape(48,48)\n",
    "        os.chdir(str(test_label[i]))\n",
    "        cv2.imwrite(s+str(counter)+g,img/np.max(img)*255)\n",
    "        os.chdir('..')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/suraj18025/SML_Project/fer2013'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "def default_loader(path):\n",
    "\treturn Image.open(path).convert('RGB')\n",
    "\n",
    "def default_flist_reader(flist):\n",
    "    \"\"\"\n",
    "    flist format: impath label\\nimpath label\\n ...(same to caffe's filelist)\n",
    "    \"\"\"\n",
    "    imlist = []\n",
    "    os.chdir(flist)\n",
    "    f=os.listdir()\n",
    "    for i in range(len(f)):\n",
    "        os.chdir(f[i])\n",
    "        g=os.listdir()\n",
    "        for j in range(len(g)):\n",
    "            imlist.append(((os.getcwd()+str('/')+str(g[j])), int(f[i])))\n",
    "\n",
    "        os.chdir('..')\n",
    "\n",
    "    return imlist\n",
    "class ImageFilelist(data.Dataset):\n",
    "\tdef __init__(self, root, flist, transform=None, target_transform=None,\n",
    "\t\t\tflist_reader=default_flist_reader, loader=default_loader):\n",
    "\t\tself.root   = root\n",
    "\t\tself.imlist = flist_reader(flist)\t\t\n",
    "\t\tself.transform = transform\n",
    "\t\tself.target_transform = target_transform\n",
    "\t\tself.loader = loader\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\timpath, target = self.imlist[index]\n",
    "\t\timg = self.loader(os.path.join(self.root,impath))\n",
    "\t\tif self.transform is not None:\n",
    "\t\t\timg = self.transform(img)\n",
    "\t\tif self.target_transform is not None:\n",
    "\t\t\ttarget = self.target_transform(target)\n",
    "\t\t\n",
    "\t\treturn img, target\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.imlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('fer2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(224),transforms.ToTensor()])\n",
    "trainloader = torch.utils.data.DataLoader(ImageFilelist(root=\"./data/\", flist=\"./data/\",\n",
    "                                                         transform=transform),batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "os.chdir('..')\n",
    "testloader = torch.utils.data.DataLoader(ImageFilelist(root=\"./testdata/\", flist=\"./testdata/\",\n",
    "                                                         transform=transform),batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_label=[]\n",
    "# for i in range(len(train_label)):\n",
    "#     t1_label.append([train_label[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t2_label=[]\n",
    "# for i in range(len(test_label)):\n",
    "#     t2_label.append([test_label[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/suraj18025/SML_Project/fer2013/testdata'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Featurs from alexnet\n",
    "# my_x = np.array(train)\n",
    "# my_y = np.array(t1_label) # another list of numpy arrays (targets)\n",
    "\n",
    "# tensor_x = torch.stack([torch.Tensor(i) for i in my_x]) # transform to torch tensors\n",
    "# tensor_y = torch.stack([torch.Tensor(i) for i in my_y])\n",
    "\n",
    "# trainset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "# my_x = np.array(test)\n",
    "# my_y = np.array(t2_label) # another list of numpy arrays (targets)\n",
    "\n",
    "# tensor_x = torch.stack([torch.Tensor(i) for i in my_x]) # transform to torch tensors\n",
    "# tensor_y = torch.stack([torch.Tensor(i) for i in my_y])\n",
    "\n",
    "# testset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "#                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (0,1,2,3,4,5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/suraj18025/.torch/models/resnet50-19c8e357.pth\n",
      "102502400it [00:03, 30107736.71it/s]\n"
     ]
    }
   ],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "train_data=[]\n",
    "train_label=[]\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    # get the inputs\n",
    "    inputs,labels=data\n",
    "    inputs,labels=inputs.to(\"cuda:0\"), labels.to(\"cuda:0\")\n",
    "\n",
    "    outputs = resnet50(inputs)\n",
    "    g=outputs.cpu().detach().numpy().tolist()\n",
    "    h=labels.cpu().numpy().tolist()\n",
    "    for j in range(len(g)):\n",
    "\n",
    "        train_data.append(g[j])\n",
    "        train_label.append(h[j])\n",
    "#         print(h[i])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://pytorch.org/docs/stable/torchvision/models.html\n",
    "# dataiter = iter(testloader)\n",
    "# images, labels = dataiter.next()\n",
    "# images,labels=images.to(\"cuda:0\"), labels.to(\"cuda:0\")\n",
    "# outputs = alexnet(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[]\n",
    "test_label=[]\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images,labels=images.to(\"cuda:0\"), labels.to(\"cuda:0\")\n",
    "        outputs = resnet50(images)\n",
    "        g=outputs.cpu().numpy().tolist()\n",
    "        h=labels.cpu().numpy().tolist()\n",
    "        for i in range(len(g)):\n",
    "            test_data.append(g[i])\n",
    "            test_label.append(h[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Shuffling of data\n",
    "main_data_set=[]\n",
    "for i in range(len(train_data)):\n",
    "\n",
    "    temp=[]\n",
    "    temp.append(train_data[i])\n",
    "    temp.append(train_label[i])\n",
    "    main_data_set.append(temp)\n",
    "X_train=random.sample(main_data_set, len(main_data_set))\n",
    "train_data=[]\n",
    "train_label=[]\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append(X_train[i][0])\n",
    "    train_label.append(X_train[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[958, 111, 1024, 1774, 1247, 831, 1233]\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "t=[0]*7\n",
    "for i in range(len(test_data)):\n",
    "    t[test_label[i]]+=1\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7178"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import csv\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegressionCV(random_state=0, solver='lbfgs',multi_class='multinomial')\n",
    "# clf = GaussianNB()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_set=train_data\n",
    "for i in range(len(train_data1)):\n",
    "    main_data_set[i]+=train_data1[i]\n",
    "\n",
    "train_data=main_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_set1=test_data\n",
    "for i in range(len(test_data1)):\n",
    "    main_data_set1[i]+=test_data1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=main_data_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "train_data1=pca.fit_transform(np.array(train_data))\n",
    "test_data1=pca.transform(np.array(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train = np.array(train_data),np.array(test_data),np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data1=copy.deepcopy(train_data)\n",
    "# test_data1=copy.deepcopy(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 8400  8401  8402 ... 41997 41998 41999] TEST: [   0    1    2 ... 8397 8398 8399]\n",
      "Accuracy  0.3401190476190476\n",
      "TRAIN: [    0     1     2 ... 41997 41998 41999] TEST: [ 8400  8401  8402 ... 16797 16798 16799]\n",
      "Accuracy  0.3386904761904762\n",
      "TRAIN: [    0     1     2 ... 41997 41998 41999] TEST: [16800 16801 16802 ... 25197 25198 25199]\n",
      "Accuracy  0.32761904761904764\n",
      "TRAIN: [    0     1     2 ... 41997 41998 41999] TEST: [25200 25201 25202 ... 33597 33598 33599]\n",
      "Accuracy  0.3302380952380952\n",
      "TRAIN: [    0     1     2 ... 33597 33598 33599] TEST: [33600 33601 33602 ... 41997 41998 41999]\n",
      "Accuracy  0.3532142857142857\n"
     ]
    }
   ],
   "source": [
    "kf=KFold(n_splits=5)\n",
    "model=[]\n",
    "score_set1=[]\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    valid_train_data, valid_test_data = X_train[train_index], X_train[test_index]\n",
    "    valid_train_label, valid_test_label = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    clf = LogisticRegressionCV(random_state=0, solver='lbfgs',multi_class='multinomial')\n",
    "    clf.fit(np.array(valid_train_data),np.array(valid_train_label))\n",
    "\n",
    "    b=clf.predict(np.array(valid_test_data))\n",
    "\n",
    "    score1=clf.score(valid_test_data,valid_test_label)\n",
    "    print(\"Accuracy \",score1)\n",
    "    score_set1.append(score1)\n",
    "    model.append(clf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation :  0.008993888073338296\n",
      "Mean of accuracy : 0.3379761904761905\n",
      "Accuracy for each validation :  [0.3401190476190476, 0.3386904761904762, 0.32761904761904764, 0.3302380952380952, 0.3532142857142857]\n",
      "Test Accuracy by best model in cross validation :  0.3390916689885762\n"
     ]
    }
   ],
   "source": [
    "score_mean=np.mean(score_set1)\n",
    "standard_dev=np.std(score_set1)\n",
    "print(\"Standard deviation : \",standard_dev)\n",
    "print(\"Mean of accuracy :\",score_mean)\n",
    "print(\"Accuracy for each validation : \",score_set1)\n",
    "best=model[np.argmax(score_set1)]\n",
    "best_score6=best.score(np.array(test_data),np.array(test_label))\n",
    "prob_dist6=best.predict_proba(test_data)\n",
    "label_pred=best.predict(test_data).tolist()\n",
    "print(\"Test Accuracy by best model in cross validation : \",best_score6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = svm.SVC(gamma=0.001)\n",
    "# clf.fit(np.array(train_data), np.array(train_label))\n",
    "# label_predict=clf.predict(np.array(test_data)).tolist()\n",
    "# score=clf.score(np.array(test_data),np.array(test_label))\n",
    "# print(\"Accuracy : \",score)\n",
    "# prob_dist=clf.predict_proba(np.array(test_data)).tolist()\n",
    "# prob_dist=np.transpose(prob_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,score_set=cross_validation_svm(train_data,train_label,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean=np.mean(np.array(score_set))\n",
    "standard_dev=np.std(np.array(score_set))\n",
    "print(\"Standard deviation : \",standard_dev)\n",
    "print(\"Mean of accuracy :\",score_mean)\n",
    "print(\"Accuracy for each validation : \",score_set)\n",
    "best=model[np.argmax(score_set)]\n",
    "best_score=best.score(test_data,test_label)\n",
    "prob_dist=best.predict_proba(test_data).tolist()\n",
    "label_pred=best.predict(test_data).tolist()\n",
    "print(\"Test Accuracy by best model in cross validation : \",best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "confusionmatrix=[]\n",
    "temp=[0,0,0,0,0,0,0]\n",
    "for i in range(7):\n",
    "    confusionmatrix.append(temp)\n",
    "    temp=copy.deepcopy(temp)\n",
    "for i in range(len(test_label)):\n",
    "    confusionmatrix[test_label[i]][label_pred[i]]=confusionmatrix[test_label[i]][label_pred[i]]+1\n",
    "seaborn.heatmap(confusionmatrix,annot=True,fmt=\"d\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve \n",
    "prob_dist=np.transpose(prob_dist)\n",
    "for i in range(7):\n",
    "    tpr,fpr=roc_design(prob_dist[i],test_label,i)\n",
    "    plt.plot(fpr, tpr ,label=\"Class\"+str(i+1))\n",
    "# roccurve(prob_dist,test_label)\n",
    "plt.xlabel(\"False +ve Rate\")\n",
    "plt.ylabel(\"True +ve Rate\")\n",
    "plt.legend()\n",
    "plt.title(\"ROC Curve for Class 1 to 7\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ROC Curve \n",
    "# # prob_dist=np.transpose(prob_dist)\n",
    "# for i in range(7):\n",
    "#     print(i)\n",
    "#     tpr,fpr=roc_design(prob_dist[i],test_label,i)\n",
    "#     plt.plot(fpr, tpr ,label=\"Class\"+str(i+1))\n",
    "# # roccurve(prob_dist,test_label)\n",
    "# plt.xlabel(\"False +ve Rate\")\n",
    "# plt.ylabel(\"True +ve Rate\")\n",
    "# plt.legend()\n",
    "# plt.title(\"ROC Curve for Class 1 to 11\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.tolist()\n",
    "test_data=test_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,score_set=cross_validation_gaussian(train_data,train_label,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean=np.mean(np.array(score_set))\n",
    "standard_dev=np.std(np.array(score_set))\n",
    "print(\"Standard deviation : \",standard_dev)\n",
    "print(\"Mean of accuracy :\",score_mean)\n",
    "print(\"Accuracy for each validation : \",score_set)\n",
    "best=model[np.argmax(score_set)]\n",
    "best_score=best.score(test_data,test_label)\n",
    "prob_dist=best.predict_proba(test_data).tolist()\n",
    "label_pred=best.predict(test_data).tolist()\n",
    "print(\"Test Accuracy by best model in cross validation : \",best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clf = GaussianNB().fit(copy.deepcopy(np.array(train_data)), copy.deepcopy(np.array(train_label)))\n",
    "\n",
    "# label_predict=clf.predict(np.array(test_data)).tolist()\n",
    "# score=clf.score(np.array(test_data),np.array(test_label))\n",
    "# print(\"Accuracy : \",score)\n",
    "# prob_dist=clf.predict_proba(np.array(test_data)).tolist()\n",
    "# prob_dist=np.transpose(prob_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "confusionmatrix=[]\n",
    "temp=[0,0,0,0,0,0,0]\n",
    "for i in range(7):\n",
    "    confusionmatrix.append(temp)\n",
    "    temp=copy.deepcopy(temp)\n",
    "for i in range(len(test_label)):\n",
    "    confusionmatrix[test_label[i]][label_pred[i]]=confusionmatrix[test_label[i]][label_pred[i]]+1\n",
    "seaborn.heatmap(confusionmatrix,annot=True,fmt=\"d\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve \n",
    "prob_dist=np.transpose(prob_dist)\n",
    "for i in range(7):\n",
    "    tpr,fpr=roc_design(prob_dist[i],test_label,i)\n",
    "    plt.plot(fpr, tpr ,label=\"Class\"+str(i+1))\n",
    "# roccurve(prob_dist,test_label)\n",
    "plt.xlabel(\"False +ve Rate\")\n",
    "plt.ylabel(\"True +ve Rate\")\n",
    "plt.legend()\n",
    "plt.title(\"ROC Curve for Class 1 to 7\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ROC Curve \n",
    "# # prob_dist=np.transpose(prob_dist)\n",
    "# for i in range(7):\n",
    "#     print(i)\n",
    "#     tpr,fpr=roc_design(prob_dist[i],test_label,i)\n",
    "#     plt.plot(fpr, tpr ,label=\"Class\"+str(i+1))\n",
    "# # roccurve(prob_dist,test_label)\n",
    "# plt.xlabel(\"False +ve Rate\")\n",
    "# plt.ylabel(\"True +ve Rate\")\n",
    "# plt.legend()\n",
    "# plt.title(\"ROC Curve for Class 1 to 11\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array(train_data)\n",
    "y_train=np.array(train_label)\n",
    "x_test=np.array(test_data)\n",
    "y_test=np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(7, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,score_set=cross_validation_logistic(train_data,train_label,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean=np.mean(np.array(score_set))\n",
    "standard_dev=np.std(np.array(score_set))\n",
    "print(\"Standard deviation : \",standard_dev)\n",
    "print(\"Mean of accuracy :\",score_mean)\n",
    "print(\"Accuracy for each validation : \",score_set)\n",
    "best=model[np.argmax(score_set)]\n",
    "best_score=best.score(test_data,test_label)\n",
    "prob_dist=best.predict_proba(test_data).tolist()\n",
    "label_pred=best.predict(test_data).tolist()\n",
    "print(\"Test Accuracy by best model in cross validation : \",best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "confusionmatrix=[]\n",
    "temp=[0,0,0,0,0,0,0]\n",
    "for i in range(7):\n",
    "    confusionmatrix.append(temp)\n",
    "    temp=copy.deepcopy(temp)\n",
    "for i in range(len(test_label)):\n",
    "    confusionmatrix[test_label[i]][label_pred[i]]=confusionmatrix[test_label[i]][label_pred[i]]+1\n",
    "seaborn.heatmap(confusionmatrix,annot=True,fmt=\"d\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve \n",
    "prob_dist=np.transpose(prob_dist)\n",
    "for i in range(7):\n",
    "    tpr,fpr=roc_design(prob_dist[i],test_label,i)\n",
    "    plt.plot(fpr, tpr ,label=\"Class\"+str(i+1))\n",
    "# roccurve(prob_dist,test_label)\n",
    "plt.xlabel(\"False +ve Rate\")\n",
    "plt.ylabel(\"True +ve Rate\")\n",
    "plt.legend()\n",
    "plt.title(\"ROC Curve for Class 1 to 7\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegressionCV().fit(copy.deepcopy(np.array(train_data)), copy.deepcopy(np.array(train_label)))\n",
    "# label_predict=clf.predict(np.array(test_data)).tolist()\n",
    "# score=clf.score(np.array(test_data),np.array(test_label))\n",
    "# print(\"Accuracy : \",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dist=clf.predictproba(np.array(test_data),np.array(test_label)).tolist()\n",
    "prob_dist=np.transpose(prob_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "confusionmatrix=[]\n",
    "temp=[0,0,0,0,0,0,0]\n",
    "for i in range(7):\n",
    "    confusionmatrix.append(temp)\n",
    "    temp=copy.deepcopy(temp)\n",
    "for i in range(len(test_label)):\n",
    "    confusionmatrix[test_label[i]][label_predict[i]]=confusionmatrix[test_label[i]][label_predict[i]]+1\n",
    "seaborn.heatmap(confusionmatrix,annot=True,fmt=\"d\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve \n",
    "# prob_dist=np.transpose(prob_dist)\n",
    "for i in range(7):\n",
    "    print(i)\n",
    "    tpr,fpr=roc_design(prob_dist[i],test_label,i)\n",
    "    plt.plot(fpr, tpr ,label=\"Class\"+str(i+1))\n",
    "# roccurve(prob_dist,test_label)\n",
    "plt.xlabel(\"False +ve Rate\")\n",
    "plt.ylabel(\"True +ve Rate\")\n",
    "plt.legend()\n",
    "plt.title(\"ROC Curve for Class 1 to 11\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enseamble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost(n,train_data1,train_label1,test_data,test_label,weights,d):\n",
    "    \n",
    "    alpha_k=[]\n",
    "    Ck=[]\n",
    "    nat=[i for i in range(len(train_data1))]\n",
    "    main_data=copy.deepcopy(train_data1)\n",
    "    main_label=copy.deepcopy(train_label1)\n",
    "    \n",
    "    for i in range(n):\n",
    "        print(i)\n",
    "#         print(\"Hello : \",i)\n",
    "        sample = choice(nat, d,p=weights,replace=False)\n",
    "#         sample = choice(nat,d,weights,replace=False)\n",
    "        train_data=[]\n",
    "        train_label=[]\n",
    "        for j in range(len(sample)):\n",
    "            train_data.append(train_data1[sample[j]])\n",
    "            train_label.append(train_label1[sample[j]])\n",
    "        \n",
    "        clf=DecisionTreeClassifier(max_depth=3,max_leaf_nodes=10)\n",
    "        clf.fit(np.array(train_data),np.array(train_label))\n",
    "        predict1=clf.predict(np.array(main_data))\n",
    "        h=labelling(predict1.tolist(),main_label)\n",
    "        train_err=clf.score(np.array(train_data),np.array(train_label))\n",
    "        train_err=1-train_err\n",
    "        alpha=0.5*np.log((1-train_err)/float(train_err))+np.log(25)\n",
    "        alpha_k.append(alpha)\n",
    "        Ck.append(clf)\n",
    "#         print(\"Hello1 : \",i)\n",
    "        for j in range(len(weights)):\n",
    "            \n",
    "            if h[j]==1:\n",
    "                \n",
    "                weights[j]=weights[j]*math.exp((-1)*alpha)\n",
    "            else:\n",
    "                weights[j]=weights[j]*math.exp(alpha)\n",
    "        w=copy.deepcopy(weights)\n",
    "        total=np.sum(w)\n",
    "        for j in range(len(weights)):\n",
    "            weights[j]=weights[j]/float(total)\n",
    "#         print(\"Hello2 : \",i)\n",
    " #For test set\n",
    "    test_predict=[]\n",
    "    for i in range(len(test_data)):\n",
    "        disc_func=[[] for i in class_label]\n",
    "        for j in range(k_max):\n",
    "            index=Ck[j].predict(np.array(test_data[i]).reshape(1,-1)).tolist()[0]\n",
    "            if disc_func[index]==[]:\n",
    "                disc_func[index].append(alpha_k[j])\n",
    "            else:\n",
    "                disc_func[index][0]+=alpha_k[j]\n",
    "        test_predict.append(disc_func.index(max(disc_func)))\n",
    "\n",
    "    test_acc=accuracy(test_predict,test_label)   \n",
    "# For train set\n",
    "    train_predict=[]\n",
    "#     for i in range(len(train_data1)):\n",
    "#         disc_func1=[[] for i in class_label]\n",
    "#         for j in range(k_max):\n",
    "#             index=Ck[j].predict(np.array(train_data1[i]).reshape(1,-1)).tolist()[0]\n",
    "#             if disc_func1[index]==[]:\n",
    "#                 disc_func1[index].append(alpha_k[j])\n",
    "#             else:\n",
    "#                 disc_func1[index][0]+=alpha_k[j]\n",
    "#         train_predict.append(disc_func1.index(max(disc_func1)))\n",
    "\n",
    "#     train_acc=accuracy(train_predict,train_label1)  \n",
    "    train_acc=0\n",
    "    return Ck,alpha_k,train_predict,test_predict,train_acc,test_acc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max=50\n",
    "d=3000\n",
    "class_label=list(set(train_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=[1/float(len(train_data)) for i in train_data]\n",
    "Ck,alpha_k,train_result,test_result,train_acc,test_acc=adaboost(k_max,train_data,train_label,test_data,test_label,weights,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy in training : \",train_acc)\n",
    "print(\"Accuracy in testing : \",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(train_data):\n",
    "    hist=[0]*256\n",
    "    for i in range(len(train_data)):\n",
    "        hist[train_data[i]]+=1\n",
    "    return hist\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1=[]\n",
    "for i in range(len(train_data)):\n",
    "    train_data1.append(histogram(train_data[i]))\n",
    "test_data1=[]\n",
    "for i in range(len(test_data)):\n",
    "    test_data1.append(histogram(test_data[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=copy.deepcopy(train_data1)\n",
    "test_data=copy.deepcopy(test_data1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,score_set=cross_validation_logistic(train_data,train_label,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean=np.mean(np.array(score_set))\n",
    "standard_dev=np.std(np.array(score_set))\n",
    "print(\"Standard deviation : \",standard_dev)\n",
    "print(\"Mean of accuracy :\",score_mean)\n",
    "print(\"Accuracy for each validation : \",score_set)\n",
    "best=model[np.argmax(score_set)]\n",
    "best_score=best.score(test_data,test_label)\n",
    "prob_dist=best.predict_proba(test_data).tolist()\n",
    "label_pred=best.predict(test_data).tolist()\n",
    "print(\"Test Accuracy by best model in cross validation : \",best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "confusionmatrix=[]\n",
    "temp=[0,0,0,0,0,0,0]\n",
    "for i in range(7):\n",
    "    confusionmatrix.append(temp)\n",
    "    temp=copy.deepcopy(temp)\n",
    "for i in range(len(test_label)):\n",
    "    confusionmatrix[test_label[i]][label_pred[i]]=confusionmatrix[test_label[i]][label_pred[i]]+1\n",
    "seaborn.heatmap(confusionmatrix,annot=True,fmt=\"d\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve \n",
    "prob_dist=np.transpose(prob_dist)\n",
    "for i in range(7):\n",
    "    print(i)\n",
    "    tpr,fpr=roc_design(prob_dist[i],test_label,i)\n",
    "    plt.plot(fpr, tpr ,label=\"Class\"+str(i+1))\n",
    "# roccurve(prob_dist,test_label)\n",
    "plt.xlabel(\"False +ve Rate\")\n",
    "plt.ylabel(\"True +ve Rate\")\n",
    "plt.legend()\n",
    "plt.title(\"ROC Curve for Class 1 to 11\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,score_set=cross_validation_gaussian(train_data,train_label,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean=np.mean(np.array(score_set))\n",
    "standard_dev=np.std(np.array(score_set))\n",
    "print(\"Standard deviation : \",standard_dev)\n",
    "print(\"Mean of accuracy :\",score_mean)\n",
    "print(\"Accuracy for each validation : \",score_set)\n",
    "best=model[np.argmax(score_set)]\n",
    "best_score=best.score(test_data,test_label)\n",
    "prob_dist=best.predict_proba(test_data).tolist()\n",
    "label_pred=best.predict(test_data).tolist()\n",
    "print(\"Test Accuracy by best model in cross validation : \",best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "confusionmatrix=[]\n",
    "temp=[0,0,0,0,0,0,0]\n",
    "for i in range(7):\n",
    "    confusionmatrix.append(temp)\n",
    "    temp=copy.deepcopy(temp)\n",
    "for i in range(len(test_label)):\n",
    "    confusionmatrix[test_label[i]][label_pred[i]]=confusionmatrix[test_label[i]][label_pred[i]]+1\n",
    "seaborn.heatmap(confusionmatrix,annot=True,fmt=\"d\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve \n",
    "prob_dist=np.transpose(prob_dist)\n",
    "for i in range(7):\n",
    "    print(i)\n",
    "    tpr,fpr=roc_design(prob_dist[i],test_label,i)\n",
    "    plt.plot(fpr, tpr ,label=\"Class\"+str(i+1))\n",
    "# roccurve(prob_dist,test_label)\n",
    "plt.xlabel(\"False +ve Rate\")\n",
    "plt.ylabel(\"True +ve Rate\")\n",
    "plt.legend()\n",
    "plt.title(\"ROC Curve for Class 1 to 11\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,score_set=cross_validation_svm(train_data,train_label,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean=np.mean(np.array(score_set))\n",
    "standard_dev=np.std(np.array(score_set))\n",
    "print(\"Standard deviation : \",standard_dev)\n",
    "print(\"Mean of accuracy :\",score_mean)\n",
    "print(\"Accuracy for each validation : \",score_set)\n",
    "best=model[np.argmax(score_set)]\n",
    "best_score=best.score(test_data,test_label)\n",
    "prob_dist=best.predict_proba(test_data).tolist()\n",
    "label_pred=best.predict(test_data).tolist()\n",
    "print(\"Test Accuracy by best model in cross validation : \",best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "confusionmatrix=[]\n",
    "temp=[0,0,0,0,0,0,0]\n",
    "for i in range(7):\n",
    "    confusionmatrix.append(temp)\n",
    "    temp=copy.deepcopy(temp)\n",
    "for i in range(len(test_label)):\n",
    "    confusionmatrix[test_label[i]][label_pred[i]]=confusionmatrix[test_label[i]][label_pred[i]]+1\n",
    "seaborn.heatmap(confusionmatrix,annot=True,fmt=\"d\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve \n",
    "prob_dist=np.transpose(prob_dist)\n",
    "for i in range(7):\n",
    "    print(i)\n",
    "    tpr,fpr=roc_design(prob_dist[i],test_label,i)\n",
    "    plt.plot(fpr, tpr ,label=\"Class\"+str(i+1))\n",
    "# roccurve(prob_dist,test_label)\n",
    "plt.xlabel(\"False +ve Rate\")\n",
    "plt.ylabel(\"True +ve Rate\")\n",
    "plt.legend()\n",
    "plt.title(\"ROC Curve for Class 1 to 11\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array(train_data)\n",
    "y_train=np.array(train_label)\n",
    "x_test=np.array(test_data)\n",
    "y_test=np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(7, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max=50\n",
    "d=3000\n",
    "class_label=list(set(train_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict,true):\n",
    "    count=0\n",
    "    for i in range(len(predict)):\n",
    "        if predict[i]==true[i]:\n",
    "            count+=1\n",
    "    return count/float(len(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=[1/float(len(train_data)) for i in train_data]\n",
    "Ck,alpha_k,train_result,test_result,train_acc,test_acc=adaboost(k_max,train_data,train_label,test_data,test_label,weights,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy in training : \",train_acc)\n",
    "print(\"Accuracy in testing : \",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "confusionmatrix=[]\n",
    "temp=[0,0,0,0,0,0,0]\n",
    "for i in range(7):\n",
    "    confusionmatrix.append(temp)\n",
    "    temp=copy.deepcopy(temp)\n",
    "for i in range(len(test_label)):\n",
    "    confusionmatrix[test_label[i]][test_result[i]]=confusionmatrix[test_label[i]][test_result[i]]+1\n",
    "seaborn.heatmap(confusionmatrix,annot=True,fmt=\"d\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
